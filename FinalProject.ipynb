{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab2041f8",
   "metadata": {},
   "source": [
    "\n",
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59494f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in d:\\software\\anaconda\\lib\\site-packages (4.6.0.66)\n",
      "Requirement already satisfied: numpy>=1.17.3 in d:\\software\\anaconda\\lib\\site-packages (from opencv-python) (1.20.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc24985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch==1.11.0 torchvision==0.12.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adf85dd",
   "metadata": {},
   "source": [
    "### Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aba5a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5c7708",
   "metadata": {},
   "source": [
    "### Functions related to image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4139364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame2tiles(frame, tile_w, tile_h, overlap_size, frameID=None):\n",
    "    '''\n",
    "    Split a complete image into smaller images with overlap\n",
    "    return: Tiles object\n",
    "    '''\n",
    "    tiles = Tiles(frameID=frameID, frame_size=frame.shape, overlap_size=overlap_size, tile_h=tile_h, tile_w=tile_w)\n",
    "    w, h = frame.shape[:2]\n",
    "    \n",
    "    # Calculate how many pieces the image should be divided into horizontally and vertically\n",
    "    stride_w = int((w - overlap_size) / tile_w) \n",
    "    stride_h = int((h - overlap_size) / tile_h)\n",
    "    tileID = 0\n",
    "    \n",
    "    # Iterate through all the tiles that should be sliced and packed into tiles objects\n",
    "    for i in range(tile_w):\n",
    "        for j in range(tile_h):\n",
    "            x1, y1 = i * stride_w, j * stride_h\n",
    "            x2, y2 = x1 + stride_w + overlap_size, y1 + stride_h + overlap_size\n",
    "            tile = Tile(frameID, tileID, (x1, y1), frame[x1:x2, y1:y2, :])\n",
    "            tiles.pushTile(tile)\n",
    "            tileID += 1\n",
    "    return tiles\n",
    "    \n",
    "class Tile:\n",
    "    '''\n",
    "    save tile info\n",
    "    '''\n",
    "    def __init__(self, frameID, tileID, start_point, tile=None): # init tile info\n",
    "        self.frameID = frameID\n",
    "        self.tileID = tileID\n",
    "        self.tile = tile\n",
    "        self.start_point = start_point\n",
    "    \n",
    "    def add_pred(self, pred): # Add prediction box information and convert it into tensor\n",
    "        a, b = self.start_point\n",
    "        self.pred = pred +torch.tensor([b, a, b, a, 0, 0])\n",
    "\n",
    "class Tiles:\n",
    "    '''\n",
    "    save tiles information, iterable\n",
    "    '''\n",
    "    def __init__(self,  frameID=0, tile_w=1, tile_h=1, overlap_size=0, frame_size=(640,640,3)): # init tiles info\n",
    "        self.tiles = list()\n",
    "        self.frameID = frameID\n",
    "        self.overlap_size = overlap_size\n",
    "        self.tile_w = tile_w\n",
    "        self.tile_h = tile_h\n",
    "        self.frame_size = frame_size\n",
    "        self.frame = np.zeros(frame_size, dtype=np.int16)\n",
    "        self.index = 0\n",
    "    \n",
    "    def pushTile(self, tile): # add a tile into tiles\n",
    "        self.tiles.append(tile)\n",
    "    \n",
    "    def merge_tiles2frame(self): # merge tiles to a frame\n",
    "        for tile in self.tiles:\n",
    "            x1, y1 = tile.start_point\n",
    "            w, h = tile.w, tile.h\n",
    "            self.frame[x1:x1+w, y1:y1+h] = tile.tile\n",
    "        return self.frame\n",
    "    \n",
    "    def __len__(self): # return tiles num\n",
    "        return len(self.tiles)\n",
    "    \n",
    "    def __getitem__(self, index): # get tile use index\n",
    "        return self.tiles[index]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self): # get next tile\n",
    "        if self.index < len(self.tiles):\n",
    "            item = self.tiles[self.index]\n",
    "            self.index += 1\n",
    "            return item\n",
    "        else:\n",
    "            self.index = 0\n",
    "            raise StopIteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4dab4c",
   "metadata": {},
   "source": [
    "### Display image tiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12611d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = np.array(Image.open('./data/images/0000001_02999_d_0000005.jpg'))\n",
    "\n",
    "# split frame to 2 x 2\n",
    "img2_tiles = frame2tiles(img2, 2, 2, 80)\n",
    "plt.figure(figsize=(8,4))\n",
    "for i,tile in enumerate(img2_tiles):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.imshow(tile.tile)\n",
    "    plt.xlabel('tile %d'%(i+1))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "# plt.savefig('data/images/frame2tiles.svg',format='svg',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcbfcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split frame to 2 x 3\n",
    "img2_tiles = frame2tiles(img2, 2, 3, 80)\n",
    "plt.figure(figsize=(8,4))\n",
    "for i,tile in enumerate(img2_tiles):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(tile.tile)\n",
    "    plt.xlabel('tile %d'%(i+1))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbc6ec7",
   "metadata": {},
   "source": [
    "### Object Detection and Prediction Result Processing Correlation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e031c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['pedestrian', 'people', 'bicycle', 'car', 'van','truck', 'tricycle', 'awning-tricycle', 'bus', 'motor', 'others']\n",
    "class YOLO: # detect model , call this class return predict bbox\n",
    "    '''\n",
    "    return a yolo detector\n",
    "    '''\n",
    "    def __init__(self, model='yolov5l-xs-1'):\n",
    "        weight_path = f'./model/tph-yolov5/weights/{model}.pt'\n",
    "\n",
    "        model = torch.hub.load('./model/tph-yolov5', 'custom', \n",
    "                            path=weight_path, source='local', force_reload=True)\n",
    "        model.amp = False\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, img, img_size=640):\n",
    "        det = self.model(img, img_size)\n",
    "        return det.pred[0].to('cpu')\n",
    "\n",
    "class Bbox_Object:\n",
    "    '''\n",
    "    bounding box info, transform xyxy format to xywh.\n",
    "    '''\n",
    "    def __init__(self, bbox, format='xyxy'):\n",
    "        if len(bbox.shape) != 2:\n",
    "            bbox.unsqueeze(dim=0)\n",
    "        self.bbox = bbox\n",
    "        self.format = format\n",
    "\n",
    "    def transfer_format(self, toformat='xyxy'):# Convert the four point form of bbox\n",
    "        if self.format == toformat:\n",
    "            return None\n",
    "        elif toformat == 'xywh':\n",
    "            w = self.bbox[:, 2] - self.bbox[:, 0]\n",
    "            h = self.bbox[:, 3] - self.bbox[:, 1]\n",
    "            self.bbox[:, 2] = w\n",
    "            self.bbox[:, 3] = h\n",
    "            self.format = format\n",
    "        else:\n",
    "            x2 = self.bbox[:, 0] + self.bbox[:, 2]\n",
    "            y2 = self.bbox[:, 1] + self.bbox[:, 3]\n",
    "            self.bbox[:, 2] = x2\n",
    "            self.bbox[:, 3] = y2\n",
    "            self.format = format\n",
    "\n",
    "def NMS(pred, iou_threshold): \n",
    "    '''\n",
    "    Non-Maximum Suppression\n",
    "    return: index\n",
    "    '''\n",
    "    tensor_pred = torch.tensor(pred)\n",
    "    bboxs = tensor_pred[:, :4]\n",
    "    scores = tensor_pred[:, 4]\n",
    "    indices = torchvision.ops.nms(bboxs, scores, iou_threshold) # delete duplicate bbox\n",
    "    return pred[indices]\n",
    "\n",
    "def drawBbox(img, pred, hide_labels=True, hide_conf=True):\n",
    "    from model.YOLOV5.utils.plots import Annotator, colors\n",
    "    '''\n",
    "    draw prediction bbox\n",
    "    return: numpy array image\n",
    "    '''\n",
    "    annotator = Annotator(np.ascontiguousarray(img), line_width=2, example=str(names))\n",
    "    for *xyxy, conf, cls in reversed(pred): # Draw each bbox on the picture\n",
    "     # Add bbox to image\n",
    "        c = int(cls)  # integer class\n",
    "        label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')\n",
    "        annotator.box_label(xyxy, label, color=colors(c, True))\n",
    "    return annotator.result()\n",
    "\n",
    "def detectTiles(tiles, size, iou_threshold, model):\n",
    "    '''\n",
    "    detect tiles one by one, combine predictions and NMS\n",
    "    return: prediction(torch tensor)\n",
    "    '''\n",
    "    for i,tile in enumerate(tiles): # Iterate over each tile\n",
    "        res = model(tile.tile, size)\n",
    "        tile.add_pred(res)\n",
    "        if i == 0:\n",
    "            pred = tile.pred\n",
    "        else:\n",
    "            pred = torch.vstack((pred, tile.pred))\n",
    "        pred = NMS(pred, iou_threshold) # delete duplicate bbox\n",
    "    return pred\n",
    "\n",
    "def detectImage(img, size, iou_threshold, model):\n",
    "    '''\n",
    "    detect a image\n",
    "    return: prediction(torch tensor)\n",
    "    '''\n",
    "    pred = model(img, size)\n",
    "    pred = NMS(pred, iou_threshold)# delete duplicate bbox\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cce974",
   "metadata": {},
   "source": [
    "### Detection effect display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee964c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO()\n",
    "pred = detectTiles(img2_tiles, 480, 0.5, model)\n",
    "res = drawBbox(img2, pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e09aa9f",
   "metadata": {},
   "source": [
    "### Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66123d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "class FPS:\n",
    "    def __init__(self):\n",
    "        # store the start time, end time, and total number of frames\n",
    "        # that were examined between the start and end intervals\n",
    "        self._start = None\n",
    "        self._end = None\n",
    "        self._numFrames = 0\n",
    "\n",
    "    def start(self):\n",
    "        # start the timer\n",
    "        self._start = datetime.datetime.now()\n",
    "        return self\n",
    "\n",
    "    def stop(self):\n",
    "        # stop the timer\n",
    "        self._end = datetime.datetime.now()\n",
    "\n",
    "    def update(self):\n",
    "        # increment the total number of frames examined during the\n",
    "        # start and end intervals\n",
    "        self._numFrames += 1\n",
    "\n",
    "    def elapsed(self):\n",
    "        # return the total number of seconds between the start and\n",
    "        # end interval\n",
    "        return (self._end - self._start).total_seconds()\n",
    "\n",
    "    def fps(self):\n",
    "        # compute the (approximate) frames per second\n",
    "        return self._numFrames / self.elapsed()\n",
    "\n",
    "    def clear(self):\n",
    "        self.__init__()\n",
    "\n",
    "def compute_iou(box1, box2):\n",
    "    \"\"\"Compute the intersection over union of two set of boxes, each box is [x1,y1,x2,y2].\n",
    "    Args:\n",
    "      box1: (tensor) bounding boxes, sized [N,4].\n",
    "      box2: (tensor) bounding boxes, sized [M,4].\n",
    "    Return:\n",
    "      (tensor) iou, sized [N,M].\n",
    "    \"\"\"\n",
    "    N = box1.size(0)\n",
    "    M = box2.size(0)\n",
    "\n",
    "    lt = torch.max(  \n",
    "        box1[:, :2].unsqueeze(1).expand(N, M, 2),  # [N,2] -> [N,1,2] -> [N,M,2]\n",
    "        box2[:, :2].unsqueeze(0).expand(N, M, 2),  # [M,2] -> [1,M,2] -> [N,M,2]\n",
    "    )\n",
    "\n",
    "    rb = torch.min(  \n",
    "        box1[:, 2:].unsqueeze(1).expand(N, M, 2),  # [N,2] -> [N,1,2] -> [N,M,2]\n",
    "        box2[:, 2:].unsqueeze(0).expand(N, M, 2),  # [M,2] -> [1,M,2] -> [N,M,2]\n",
    "    )\n",
    "\n",
    "    wh = rb - lt  \n",
    "    wh[wh < 0] = 0  \n",
    "    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n",
    "\n",
    "    area1 = (box1[:, 2]-box1[:, 0]) * (box1[:, 3]-box1[:, 1])  # [N,]\n",
    "    area2 = (box2[:, 2]-box2[:, 0]) * (box2[:, 3]-box2[:, 1])  # [M,]\n",
    "    area1 = area1.unsqueeze(1).expand_as(inter)  # [N,] -> [N,1] -> [N,M]\n",
    "    area2 = area2.unsqueeze(0).expand_as(inter)  # [M,] -> [1,M] -> [N,M]\n",
    "\n",
    "    iou = inter / (area1 + area2 - inter)\n",
    "    return iou\n",
    "\n",
    "def compute_average_iou(iou_matrix):\n",
    "    if iou_matrix.shape[1] == 0:\n",
    "        return 0\n",
    "    return (iou_matrix.max(0).values.sum() / len(iou_matrix)).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb86f4fe",
   "metadata": {},
   "source": [
    "### Test algorithm performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710a80bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './model/tph-yolov5/data/VisDrone/VisDrone2019-DET-val'\n",
    "labels_txt = [root + '/annotations/' + name for name in os.listdir('./model/tph-yolov5/data/VisDrone/VisDrone2019-DET-val/annotations/')]\n",
    "img_file = [root + '/images/' + name for name in os.listdir('./model/tph-yolov5/data/VisDrone/VisDrone2019-DET-val/images/')]\n",
    "\n",
    "img_iou = []\n",
    "tiles_iou = []\n",
    "img_time = []\n",
    "tiles_time = []\n",
    "fps = FPS()\n",
    "for i in range(len(labels_txt)):\n",
    "    iou_threshold = 0.5\n",
    "    img_label = torch.tensor(np.genfromtxt(labels_txt[i], delimiter=',').reshape((-1, 8)))[:, :4]\n",
    "    label_bbox = Bbox_Object(img_label, 'xywh')\n",
    "    label_bbox.transfer_format('xyxy')\n",
    "    img = np.array(Image.open(img_file[i]))\n",
    "    \n",
    "    fps.start()\n",
    "    tiles = frame2tiles(img, 2, 3, 80)\n",
    "    tiles_pred = detectTiles(tiles, 480, iou_threshold, model)\n",
    "    tiles_bbox = Bbox_Object(tiles_pred[:, :4], 'xyxy')\n",
    "    tiles_iou_matrix = compute_iou(label_bbox.bbox, tiles_bbox.bbox)\n",
    "    tiles_avg_iou = compute_average_iou(tiles_iou_matrix)\n",
    "    tiles_iou.append(tiles_avg_iou)\n",
    "    fps.stop()\n",
    "    tiles_time.append(fps.elapsed())\n",
    "    \n",
    "    fps.start()\n",
    "    img_pred = detectImage(img, 960, iou_threshold, model)\n",
    "    img_bbox = Bbox_Object(img_pred[:, :4], 'xyxy')\n",
    "    img_iou_matrix = compute_iou(label_bbox.bbox, img_bbox.bbox)\n",
    "    img_avg_iou = compute_average_iou(img_iou_matrix)\n",
    "    img_iou.append(img_avg_iou)\n",
    "    fps.stop()\n",
    "    img_time.append(fps.elapsed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17121633",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(2)\n",
    "tiles = [np.mean(tiles_iou), np.mean(img_iou)]\n",
    "img = [np.mean(tiles_time), np.mean(img_time)]\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.bar(x, tiles, width=0.2, label = 'Tiles',color = '#FFCCCC');\n",
    "ax1.bar(x + 0.2, img, width=0.2, label = 'Image',color = '#6699CC')\n",
    "ax1.set_ylabel('IoU');\n",
    "ax2 = ax1.twinx() # this is the important function\n",
    "plt.xticks(x+0.1, ['IoU', 'Time'])\n",
    "ax2.set_ylabel('Time (s)');\n",
    "ax1.legend()\n",
    "plt.show()\n",
    "# fig.savefig('./data/images/acc.svg', format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee30ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
